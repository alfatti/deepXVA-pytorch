{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep BSDE Solver for European Call Option Pricing\n",
    "\n",
    "This notebook demonstrates the use of deep learning to solve Backward Stochastic Differential Equations (BSDEs) for pricing European call options.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We use a neural network to approximate the solution to the BSDE that represents the call option pricing problem:\n",
    "\n",
    "- **Underlying dynamics**: Geometric Brownian Motion\n",
    "- **Option**: European Call with strike K\n",
    "- **Method**: Deep BSDE solver (Han, Jentzen, E, 2018)\n",
    "\n",
    "The BSDE formulation:\n",
    "- $dY_t = -f(t, X_t, Y_t, Z_t)dt + Z_t dW_t$\n",
    "- $Y_T = g(X_T) = \\max(X_T - K, 0)$\n",
    "\n",
    "where $f(t, X_t, Y_t, Z_t) = -rY_t$ for option pricing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install torch numpy scipy matplotlib pandas munch tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.stats import multivariate_normal as normal\n",
    "from scipy.stats import norm\n",
    "import munch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the Equation Classes\n",
    "\n",
    "We define the base equation class and the specific call option equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Equation(object):\n",
    "    \"\"\"Base class for defining PDE related function.\"\"\"\n",
    "\n",
    "    def __init__(self, eqn_config):\n",
    "        self.dim = eqn_config.dim\n",
    "        self.total_time = eqn_config.total_time\n",
    "        self.num_time_interval = eqn_config.num_time_interval\n",
    "        self.delta_t = self.total_time / self.num_time_interval\n",
    "        self.sqrt_delta_t = np.sqrt(self.delta_t)\n",
    "        self.y_init = None\n",
    "\n",
    "    def sample(self, num_sample):\n",
    "        \"\"\"Sample forward SDE.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def f_torch(self, t, x, y, z):\n",
    "        \"\"\"Generator function in the PDE.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def g_torch(self, t, x):\n",
    "        \"\"\"Terminal condition of the PDE.\"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CallOption(Equation):\n",
    "    \"\"\"European Call Option under Black-Scholes model.\"\"\"\n",
    "    \n",
    "    def __init__(self, eqn_config):\n",
    "        super(CallOption, self).__init__(eqn_config)\n",
    "        self.strike = eqn_config.strike\n",
    "        self.x_init = np.ones(self.dim) * eqn_config.x_init\n",
    "        self.sigma = eqn_config.sigma\n",
    "        self.r = eqn_config.r\n",
    "        self.useExplicit = False\n",
    "\n",
    "    def sample(self, num_sample):\n",
    "        \"\"\"Sample paths using Euler-Maruyama or exact solution.\"\"\"\n",
    "        dw_sample = normal.rvs(size=[num_sample, self.dim, self.num_time_interval]) * self.sqrt_delta_t\n",
    "        \n",
    "        if self.dim == 1:\n",
    "            dw_sample = np.expand_dims(dw_sample, axis=0)\n",
    "            dw_sample = np.swapaxes(dw_sample, 0, 1)\n",
    "\n",
    "        x_sample = np.zeros([num_sample, self.dim, self.num_time_interval + 1])\n",
    "        x_sample[:, :, 0] = np.ones([num_sample, self.dim]) * self.x_init\n",
    "\n",
    "        if self.useExplicit:\n",
    "            factor = np.exp((self.r - (self.sigma**2) / 2) * self.delta_t)\n",
    "            for i in range(self.num_time_interval):\n",
    "                x_sample[:, :, i + 1] = (factor * np.exp(self.sigma * dw_sample[:, :, i])) * x_sample[:, :, i]\n",
    "        else:\n",
    "            for i in range(self.num_time_interval):\n",
    "                x_sample[:, :, i + 1] = ((1 + self.r * self.delta_t) * x_sample[:, :, i] + \n",
    "                                         (self.sigma * x_sample[:, :, i] * dw_sample[:, :, i]))\n",
    "\n",
    "        return dw_sample, x_sample\n",
    "\n",
    "    def f_torch(self, t, x, y, z):\n",
    "        \"\"\"Generator function: -rY for option pricing.\"\"\"\n",
    "        return -self.r * y\n",
    "\n",
    "    def g_torch(self, t, x):\n",
    "        \"\"\"Terminal condition: max(S_T - K, 0).\"\"\"\n",
    "        return torch.maximum(x - self.strike, torch.zeros_like(x))\n",
    "\n",
    "    def SolExact(self, t, x):\n",
    "        \"\"\"Black-Scholes exact solution.\"\"\"\n",
    "        tau = self.total_time - t\n",
    "        d1 = (np.log(x / self.strike) + (self.r + 0.5 * self.sigma**2) * tau) / (self.sigma * np.sqrt(tau))\n",
    "        d2 = d1 - self.sigma * np.sqrt(tau)\n",
    "        \n",
    "        call = x * norm.cdf(d1) - self.strike * np.exp(-self.r * tau) * norm.cdf(d2)\n",
    "        return call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the Neural Network Architecture\n",
    "\n",
    "The neural network approximates the gradient $Z_t$ at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELTA_CLIP = 50.0\n",
    "\n",
    "class FeedForwardSubNet(nn.Module):\n",
    "    \"\"\"Feedforward subnet for approximating Z at each time step.\"\"\"\n",
    "    \n",
    "    def __init__(self, config, dim):\n",
    "        super(FeedForwardSubNet, self).__init__()\n",
    "        \n",
    "        num_hiddens = config.net_config.num_hiddens\n",
    "        \n",
    "        if config.net_config.dtype == \"float64\":\n",
    "            self.dtype = torch.float64\n",
    "        else:\n",
    "            self.dtype = torch.float32\n",
    "        \n",
    "        # Batch normalization layers\n",
    "        self.bn_layers = nn.ModuleList([\n",
    "            nn.BatchNorm1d(dim if i == 0 else num_hiddens[i-1] if i <= len(num_hiddens) else dim,\n",
    "                          momentum=0.01, eps=1e-6)\n",
    "            for i in range(len(num_hiddens) + 2)\n",
    "        ])\n",
    "        \n",
    "        # Dense layers\n",
    "        self.dense_layers = nn.ModuleList([\n",
    "            nn.Linear(dim if i == 0 else num_hiddens[i-1], num_hiddens[i], bias=False)\n",
    "            for i in range(len(num_hiddens))\n",
    "        ])\n",
    "        \n",
    "        # Final output layer\n",
    "        input_dim = num_hiddens[-1] if num_hiddens else dim\n",
    "        self.dense_layers.append(nn.Linear(input_dim, dim, bias=True))\n",
    "\n",
    "    def forward(self, x, training):\n",
    "        \"\"\"Forward pass: bn -> (dense -> bn -> relu) * n -> dense.\"\"\"\n",
    "        if training:\n",
    "            self.train()\n",
    "        else:\n",
    "            self.eval()\n",
    "        \n",
    "        x = self.bn_layers[0](x)\n",
    "        \n",
    "        for i in range(len(self.dense_layers) - 1):\n",
    "            x = self.dense_layers[i](x)\n",
    "            x = self.bn_layers[i + 1](x)\n",
    "            x = torch.relu(x)\n",
    "        \n",
    "        x = self.dense_layers[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonsharedModel(nn.Module):\n",
    "    \"\"\"Main BSDE solver model.\"\"\"\n",
    "    \n",
    "    def __init__(self, config, bsde):\n",
    "        super(NonsharedModel, self).__init__()\n",
    "        self.config = config\n",
    "        self.eqn_config = config.eqn_config\n",
    "        self.net_config = config.net_config\n",
    "        self.bsde = bsde\n",
    "        self.dim = bsde.dim\n",
    "        \n",
    "        if self.net_config.dtype == \"float64\":\n",
    "            self.dtype = torch.float64\n",
    "        else:\n",
    "            self.dtype = torch.float32\n",
    "        \n",
    "        # Learnable initial value Y_0\n",
    "        self.y_init = nn.Parameter(\n",
    "            torch.tensor(\n",
    "                np.random.uniform(low=self.net_config.y_init_range[0],\n",
    "                                high=self.net_config.y_init_range[1],\n",
    "                                size=[1]),\n",
    "                dtype=self.dtype\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Learnable initial gradient Z_0\n",
    "        self.z_init = nn.Parameter(\n",
    "            torch.tensor(\n",
    "                np.random.uniform(low=-.1, high=.1, size=[1, self.eqn_config.dim]),\n",
    "                dtype=self.dtype\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Subnet at each time step (except last)\n",
    "        self.subnet = nn.ModuleList([\n",
    "            FeedForwardSubNet(config, bsde.dim)\n",
    "            for _ in range(self.bsde.num_time_interval - 1)\n",
    "        ])\n",
    "\n",
    "    def forward(self, inputs, training):\n",
    "        \"\"\"Forward simulation of the BSDE.\"\"\"\n",
    "        dw, x = inputs\n",
    "        time_stamp = np.arange(0, self.eqn_config.num_time_interval) * self.bsde.delta_t\n",
    "        \n",
    "        batch_size = dw.shape[0]\n",
    "        all_one_vec = torch.ones(batch_size, 1, dtype=self.dtype, device=dw.device)\n",
    "        \n",
    "        y = all_one_vec * self.y_init\n",
    "        z = torch.matmul(all_one_vec, self.z_init)\n",
    "        \n",
    "        for t in range(0, self.bsde.num_time_interval - 1):\n",
    "            f_val = self.bsde.f_torch(time_stamp[t], x[:, :, t], y, z)\n",
    "            y = y - self.bsde.delta_t * f_val + torch.sum(z * dw[:, :, t], 1, keepdim=True)\n",
    "            z = self.subnet[t](x[:, :, t + 1], training) / self.bsde.dim\n",
    "        \n",
    "        # Terminal time step\n",
    "        f_val = self.bsde.f_torch(time_stamp[-1], x[:, :, -2], y, z)\n",
    "        y = y - self.bsde.delta_t * f_val + torch.sum(z * dw[:, :, -1], 1, keepdim=True)\n",
    "        \n",
    "        return y\n",
    "\n",
    "    def simulate_path(self, sample_data):\n",
    "        \"\"\"Simulate the entire path Y_0, Y_1, ..., Y_T.\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            dw, x = sample_data\n",
    "            \n",
    "            dw_t = torch.tensor(dw, dtype=self.dtype, device=self.y_init.device)\n",
    "            x_t = torch.tensor(x, dtype=self.dtype, device=self.y_init.device)\n",
    "            \n",
    "            time_stamp = np.arange(0, self.eqn_config.num_time_interval) * self.bsde.delta_t\n",
    "            batch_size = dw_t.shape[0]\n",
    "            all_one_vec = torch.ones(batch_size, 1, dtype=self.dtype, device=dw_t.device)\n",
    "            \n",
    "            y = all_one_vec * self.y_init\n",
    "            z = torch.matmul(all_one_vec, self.z_init)\n",
    "            \n",
    "            history = [y.unsqueeze(-1)]\n",
    "            \n",
    "            for t in range(0, self.bsde.num_time_interval - 1):\n",
    "                f_val = self.bsde.f_torch(time_stamp[t], x_t[:, :, t], y, z)\n",
    "                y = y - self.bsde.delta_t * f_val + torch.sum(z * dw_t[:, :, t], 1, keepdim=True)\n",
    "                history.append(y.unsqueeze(-1))\n",
    "                z = self.subnet[t](x_t[:, :, t + 1], False) / self.bsde.dim\n",
    "            \n",
    "            # Terminal time\n",
    "            f_val = self.bsde.f_torch(time_stamp[-1], x_t[:, :, -2], y, z)\n",
    "            y = y - self.bsde.delta_t * f_val + torch.sum(z * dw_t[:, :, -1], 1, keepdim=True)\n",
    "            history.append(y.unsqueeze(-1))\n",
    "            \n",
    "            history = torch.cat(history, dim=-1)\n",
    "        \n",
    "        return history.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define the Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSDESolver(object):\n",
    "    \"\"\"The BSDE solver using deep neural networks.\"\"\"\n",
    "    \n",
    "    def __init__(self, config, bsde):\n",
    "        self.eqn_config = config.eqn_config\n",
    "        self.net_config = config.net_config\n",
    "        self.bsde = bsde\n",
    "        \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        if self.net_config.dtype == \"float64\":\n",
    "            self.dtype = torch.float64\n",
    "            torch.set_default_dtype(torch.float64)\n",
    "        else:\n",
    "            self.dtype = torch.float32\n",
    "            torch.set_default_dtype(torch.float32)\n",
    "        \n",
    "        self.model = NonsharedModel(config, bsde).to(self.device)\n",
    "        \n",
    "        self.lr_values = self.net_config.lr_values\n",
    "        self.lr_boundaries = self.net_config.lr_boundaries\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.net_config.lr_values[0], eps=1e-8)\n",
    "\n",
    "    def get_lr(self, step):\n",
    "        \"\"\"Piecewise constant learning rate schedule.\"\"\"\n",
    "        for i, boundary in enumerate(self.lr_boundaries):\n",
    "            if step < boundary:\n",
    "                return self.lr_values[i]\n",
    "        return self.lr_values[-1]\n",
    "\n",
    "    def loss_fn(self, inputs, training):\n",
    "        \"\"\"Compute the loss function.\"\"\"\n",
    "        dw, x = inputs\n",
    "        \n",
    "        dw_t = torch.tensor(dw, dtype=self.dtype, device=self.device)\n",
    "        x_t = torch.tensor(x, dtype=self.dtype, device=self.device)\n",
    "        \n",
    "        y_terminal = self.model((dw_t, x_t), training)\n",
    "        g_terminal = self.bsde.g_torch(self.bsde.total_time, x_t[:, :, -1])\n",
    "        \n",
    "        delta = y_terminal - g_terminal\n",
    "        \n",
    "        # Huber-like loss with clipping\n",
    "        loss = torch.mean(torch.where(torch.abs(delta) < DELTA_CLIP,\n",
    "                                      torch.square(delta),\n",
    "                                      2 * DELTA_CLIP * torch.abs(delta) - DELTA_CLIP ** 2))\n",
    "        \n",
    "        # Penalty for Y_0 outside initial range\n",
    "        loss += 1000 * (torch.maximum(self.model.y_init[0] - self.net_config.y_init_range[1],\n",
    "                                     torch.tensor(0., dtype=self.dtype, device=self.device)) +\n",
    "                       torch.maximum(self.net_config.y_init_range[0] - self.model.y_init[0],\n",
    "                                    torch.tensor(0., dtype=self.dtype, device=self.device)))\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Training loop.\"\"\"\n",
    "        start_time = time.time()\n",
    "        training_history = []\n",
    "        valid_data = self.bsde.sample(self.net_config.valid_size)\n",
    "\n",
    "        for step in tqdm(range(self.net_config.num_iterations + 1)):\n",
    "            # Update learning rate\n",
    "            current_lr = self.get_lr(step)\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = current_lr\n",
    "            \n",
    "            # Logging\n",
    "            if step % self.net_config.logging_frequency == 0:\n",
    "                loss = self.loss_fn(valid_data, training=False).item()\n",
    "                y_init = self.model.y_init.detach().cpu().numpy()[0]\n",
    "                elapsed_time = time.time() - start_time\n",
    "                training_history.append([step, loss, y_init, elapsed_time])\n",
    "                \n",
    "                if self.net_config.verbose:\n",
    "                    print(f\"step: {step:5d}, loss: {loss:.4e}, Y0: {y_init:.4e}, elapsed time: {int(elapsed_time):3d}s\")\n",
    "            \n",
    "            # Training step\n",
    "            self.model.train()\n",
    "            self.optimizer.zero_grad()\n",
    "            train_data = self.bsde.sample(self.net_config.batch_size)\n",
    "            loss = self.loss_fn(train_data, training=True)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        return np.array(training_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure the Problem\n",
    "\n",
    "Set up the call option pricing problem parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem parameters\n",
    "dim = 1                    # Dimension (1D for single asset)\n",
    "P = 2048                   # Number of Monte Carlo paths for evaluation\n",
    "batch_size = 64            # Training batch size\n",
    "total_time = 1.0           # Maturity (1 year)\n",
    "num_time_interval = 200    # Number of time steps\n",
    "strike = 100               # Strike price\n",
    "r = 0.01                   # Risk-free rate (1%)\n",
    "sigma = 0.25               # Volatility (25%)\n",
    "x_init = 100               # Initial stock price\n",
    "\n",
    "config = {\n",
    "    \"eqn_config\": {\n",
    "        \"_comment\": \"European call option\",\n",
    "        \"eqn_name\": \"CallOption\",\n",
    "        \"total_time\": total_time,\n",
    "        \"dim\": dim,\n",
    "        \"num_time_interval\": num_time_interval,\n",
    "        \"strike\": strike,\n",
    "        \"r\": r,\n",
    "        \"sigma\": sigma,\n",
    "        \"x_init\": x_init,\n",
    "    },\n",
    "    \"net_config\": {\n",
    "        \"y_init_range\": [9, 11],                      # Initial guess range for option price\n",
    "        \"num_hiddens\": [dim + 20, dim + 20],          # Hidden layer sizes\n",
    "        \"lr_values\": [5e-2, 5e-3],                    # Learning rates\n",
    "        \"lr_boundaries\": [2000],                      # LR schedule boundaries\n",
    "        \"num_iterations\": 4000,                       # Training iterations\n",
    "        \"batch_size\": batch_size,\n",
    "        \"valid_size\": 1024,                           # Validation set size\n",
    "        \"logging_frequency\": 100,                     # Log every N steps\n",
    "        \"dtype\": \"float64\",                           # Use double precision\n",
    "        \"verbose\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "config = munch.munchify(config)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Stock price S0 = ${x_init}\")\n",
    "print(f\"  Strike K = ${strike}\")\n",
    "print(f\"  Volatility σ = {sigma*100}%\")\n",
    "print(f\"  Risk-free rate r = {r*100}%\")\n",
    "print(f\"  Maturity T = {total_time} year\")\n",
    "print(f\"  Time steps = {num_time_interval}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compute Black-Scholes Exact Solution\n",
    "\n",
    "For comparison, we compute the exact Black-Scholes price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the equation\n",
    "bsde = CallOption(config.eqn_config)\n",
    "\n",
    "# Compute exact Black-Scholes price\n",
    "exact_price = bsde.SolExact(0, x_init)\n",
    "\n",
    "print(f\"\\nBlack-Scholes Exact Price: ${exact_price:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train the Deep BSDE Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize solver\n",
    "bsde_solver = BSDESolver(config, bsde)\n",
    "\n",
    "print(f\"\\nModel has {sum(p.numel() for p in bsde_solver.model.parameters())} parameters\")\n",
    "print(f\"Training on {bsde_solver.device}\\n\")\n",
    "\n",
    "# Train the model\n",
    "training_history = bsde_solver.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyze Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract training history\n",
    "steps = training_history[:, 0]\n",
    "losses = training_history[:, 1]\n",
    "y0_values = training_history[:, 2]\n",
    "times = training_history[:, 3]\n",
    "\n",
    "# Final results\n",
    "final_y0 = y0_values[-1]\n",
    "final_loss = losses[-1]\n",
    "error = abs(final_y0 - exact_price)\n",
    "relative_error = error / exact_price * 100\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINAL RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Black-Scholes Price:    ${exact_price:.6f}\")\n",
    "print(f\"Deep Solver Price (Y0): ${final_y0:.6f}\")\n",
    "print(f\"Absolute Error:         ${error:.6f}\")\n",
    "print(f\"Relative Error:         {relative_error:.4f}%\")\n",
    "print(f\"Final Loss:             {final_loss:.6e}\")\n",
    "print(f\"Total Training Time:    {times[-1]:.1f}s\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Plot Training Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Loss curve\n",
    "axes[0].semilogy(steps, losses, 'b-', linewidth=2, label='Training Loss')\n",
    "axes[0].set_xlabel('Iteration', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training Loss Convergence', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend(fontsize=11)\n",
    "\n",
    "# Plot 2: Y0 convergence\n",
    "axes[1].plot(steps, y0_values, 'g-', linewidth=2, label='Deep Solver Y0')\n",
    "axes[1].axhline(y=exact_price, color='r', linestyle='--', linewidth=2, label='Black-Scholes Exact')\n",
    "axes[1].set_xlabel('Iteration', fontsize=12)\n",
    "axes[1].set_ylabel('Option Price ($)', fontsize=12)\n",
    "axes[1].set_title('Y0 (Option Price) Convergence', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Simulate Exposure Profiles\n",
    "\n",
    "Simulate the option value along paths to compute Expected Positive Exposure (EPE) and Expected Negative Exposure (ENE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate paths\n",
    "print(\"Simulating exposure paths...\")\n",
    "simulations = bsde_solver.model.simulate_path(bsde.sample(P))\n",
    "\n",
    "# Calculate discounted exposures\n",
    "time_stamp = np.linspace(0, 1, num_time_interval + 1)\n",
    "discount_factors = np.exp(-r * time_stamp)\n",
    "\n",
    "epe = np.mean(discount_factors * np.maximum(simulations, 0), axis=0)\n",
    "ene = np.mean(discount_factors * np.minimum(simulations, 0), axis=0)\n",
    "\n",
    "# Exact solution at each time point\n",
    "epe_exact = np.array([exact_price] + [exact_price for s in time_stamp[1:]])\n",
    "ene_exact = np.zeros_like(time_stamp)\n",
    "\n",
    "print(f\"Simulated {P} paths with {num_time_interval} time steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Plot Exposure Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# Plot EPE\n",
    "ax.plot(time_stamp, epe_exact, 'b--', linewidth=2.5, label='DEPE (Exact)', alpha=0.8)\n",
    "ax.plot(time_stamp, epe[0], 'b-', linewidth=2, label='DEPE (Deep Solver)', alpha=0.9)\n",
    "\n",
    "# Plot ENE\n",
    "ax.plot(time_stamp, ene_exact, 'r--', linewidth=2.5, label='DNPE (Exact)', alpha=0.8)\n",
    "ax.plot(time_stamp, ene[0], 'r-', linewidth=2, label='DNPE (Deep Solver)', alpha=0.9)\n",
    "\n",
    "ax.set_xlabel('Time (years)', fontsize=13)\n",
    "ax.set_ylabel('Discounted Exposure ($)', fontsize=13)\n",
    "ax.set_title('Expected Positive and Negative Exposure Profiles\\nCall Option (K=$100, S0=$100, σ=25%, r=1%)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=12, loc='best')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Statistical Analysis of Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics at maturity\n",
    "terminal_values = simulations[:, 0, -1]\n",
    "\n",
    "print(\"\\nStatistics at Maturity (T=1):\")\n",
    "print(f\"  Mean:     ${np.mean(terminal_values):.4f}\")\n",
    "print(f\"  Std Dev:  ${np.std(terminal_values):.4f}\")\n",
    "print(f\"  Min:      ${np.min(terminal_values):.4f}\")\n",
    "print(f\"  Max:      ${np.max(terminal_values):.4f}\")\n",
    "print(f\"  Median:   ${np.median(terminal_values):.4f}\")\n",
    "\n",
    "# Percentiles\n",
    "percentiles = [5, 25, 50, 75, 95]\n",
    "print(\"\\nPercentiles:\")\n",
    "for p in percentiles:\n",
    "    print(f\"  {p}th:     ${np.percentile(terminal_values, p):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Plot Distribution of Terminal Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Histogram\n",
    "n, bins, patches = ax.hist(terminal_values, bins=50, density=True, alpha=0.7, \n",
    "                           color='steelblue', edgecolor='black', linewidth=0.5)\n",
    "\n",
    "# Add exact price line\n",
    "ax.axvline(exact_price, color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Black-Scholes: ${exact_price:.2f}')\n",
    "ax.axvline(np.mean(terminal_values), color='green', linestyle='-', linewidth=2,\n",
    "           label=f'Deep Solver Mean: ${np.mean(terminal_values):.2f}')\n",
    "\n",
    "ax.set_xlabel('Option Value at Maturity ($)', fontsize=12)\n",
    "ax.set_ylabel('Probability Density', fontsize=12)\n",
    "ax.set_title('Distribution of Terminal Option Values', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with training history\n",
    "df_training = pd.DataFrame(training_history, columns=['Step', 'Loss', 'Y0', 'Time (s)'])\n",
    "\n",
    "# Create DataFrame with exposure profiles\n",
    "df_exposure = pd.DataFrame({\n",
    "    'Time': time_stamp,\n",
    "    'EPE_Exact': epe_exact,\n",
    "    'EPE_DeepSolver': epe[0],\n",
    "    'ENE_Exact': ene_exact,\n",
    "    'ENE_DeepSolver': ene[0]\n",
    "})\n",
    "\n",
    "# Create DataFrame with simulation paths (first 10 paths)\n",
    "df_simulations = pd.DataFrame(simulations[:10, 0, :].T, \n",
    "                              columns=[f'Path_{i+1}' for i in range(10)])\n",
    "df_simulations.insert(0, 'Time', time_stamp)\n",
    "\n",
    "print(\"\\nDataFrames created:\")\n",
    "print(f\"  Training history: {df_training.shape}\")\n",
    "print(f\"  Exposure profiles: {df_exposure.shape}\")\n",
    "print(f\"  Sample paths: {df_simulations.shape}\")\n",
    "\n",
    "# Display sample of training history\n",
    "print(\"\\nTraining History (last 5 rows):\")\n",
    "display(df_training.tail())\n",
    "\n",
    "# Display exposure profile\n",
    "print(\"\\nExposure Profile (first 10 rows):\")\n",
    "display(df_exposure.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Save Results to Excel (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save results\n",
    "# with pd.ExcelWriter('call_option_results.xlsx') as writer:\n",
    "#     df_training.to_excel(writer, sheet_name='Training History', index=False)\n",
    "#     df_exposure.to_excel(writer, sheet_name='Exposure Profiles', index=False)\n",
    "#     df_simulations.to_excel(writer, sheet_name='Sample Paths', index=False)\n",
    "# \n",
    "# print(\"Results saved to 'call_option_results.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Model Information and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nProblem Configuration:\")\n",
    "print(f\"  Type:              European Call Option\")\n",
    "print(f\"  Dimension:         {dim}\")\n",
    "print(f\"  Time intervals:    {num_time_interval}\")\n",
    "print(f\"  Total time:        {total_time} year\")\n",
    "print(f\"\\nMarket Parameters:\")\n",
    "print(f\"  Initial price:     ${x_init}\")\n",
    "print(f\"  Strike:            ${strike}\")\n",
    "print(f\"  Volatility:        {sigma*100}%\")\n",
    "print(f\"  Risk-free rate:    {r*100}%\")\n",
    "print(f\"\\nNetwork Architecture:\")\n",
    "print(f\"  Hidden layers:     {config.net_config.num_hiddens}\")\n",
    "print(f\"  Parameters:        {sum(p.numel() for p in bsde_solver.model.parameters()):,}\")\n",
    "print(f\"  Dtype:             {config.net_config.dtype}\")\n",
    "print(f\"  Device:            {bsde_solver.device}\")\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Iterations:        {config.net_config.num_iterations}\")\n",
    "print(f\"  Batch size:        {config.net_config.batch_size}\")\n",
    "print(f\"  Learning rates:    {config.net_config.lr_values}\")\n",
    "print(f\"  LR boundaries:     {config.net_config.lr_boundaries}\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Black-Scholes:     ${exact_price:.6f}\")\n",
    "print(f\"  Deep Solver:       ${final_y0:.6f}\")\n",
    "print(f\"  Absolute Error:    ${error:.6f}\")\n",
    "print(f\"  Relative Error:    {relative_error:.4f}%\")\n",
    "print(f\"  Training time:     {times[-1]:.1f}s\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Deep BSDE Solver**: Neural network approximation of the solution to backward stochastic differential equations\n",
    "2. **Call Option Pricing**: Application to European call option under Black-Scholes model\n",
    "3. **Validation**: Comparison with exact Black-Scholes formula\n",
    "4. **Exposure Analysis**: Computation of expected positive and negative exposure profiles\n",
    "\n",
    "The deep learning approach successfully approximates the option price with high accuracy, validating the method for more complex problems where analytical solutions are not available.\n",
    "\n",
    "### References\n",
    "\n",
    "- Han, J., Jentzen, A., & E, W. (2018). Solving high-dimensional partial differential equations using deep learning. PNAS.\n",
    "- Gnoatto, A., Picarelli, A., & Reisinger, C. (2020). Deep xVA solver. arXiv:2005.02633."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
